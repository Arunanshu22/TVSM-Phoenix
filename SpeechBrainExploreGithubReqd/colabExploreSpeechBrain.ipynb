{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7zuMhTb8a4Z",
        "outputId": "b757f83e-e0df-4045-c50f-3a9492492bab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.0-py3-none-any.whl (760 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/760.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/760.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.1/760.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (23.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.11.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0.1)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n",
            "Installing collected packages: ruamel.yaml.clib, ruamel.yaml, hyperpyyaml, speechbrain\n",
            "Successfully installed hyperpyyaml-1.2.2 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 speechbrain-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install speechbrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc-Olgsd9b5-",
        "outputId": "0f1339dd-7c3e-4394-9dd8-96fa338f81ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /ArticleUnBroken.zip\n",
            "  inflating: ArticleUnBroken/abhishektestarticle.wav  \n",
            "  inflating: ArticleUnBroken/anirbantestarticle.wav  \n",
            "  inflating: ArticleUnBroken/aruntestarticle.wav  \n",
            "  inflating: ArticleUnBroken/shivam_gmm.wav  \n",
            "  inflating: ArticleUnBroken/sunamdhatestarticle.wav  \n"
          ]
        }
      ],
      "source": [
        "!unzip /ArticleUnBroken.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOk6SDzk8QSj"
      },
      "outputs": [],
      "source": [
        "import speechbrain\n",
        "import torchaudio\n",
        "from speechbrain.inference.speaker import EncoderClassifier\n",
        "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
        "signal, fs =torchaudio.load('/content/ArticleUnBroken/aruntestarticle.wav')\n",
        "embeddings = classifier.encode_batch(signal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPulJOe7ylcy"
      },
      "outputs": [],
      "source": [
        "import speechbrain\n",
        "import torchaudio\n",
        "from speechbrain.inference.speaker import EncoderClassifier\n",
        "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
        "signal, fs =torchaudio.load('/content/audio_5sec/Anirban/10.wav')\n",
        "embeddings = classifier.encode_batch(signal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12080\\4030946528.py:2: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  str(torchaudio.get_audio_backend())\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'None'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchaudio\n",
        "str(torchaudio.get_audio_backend())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in c:\\users\\dell\\desktop\\edu\\internship\\tvsm_speechbrain_explore\\lib\\site-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\dell\\desktop\\edu\\internship\\tvsm_speechbrain_explore\\lib\\site-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\dell\\desktop\\edu\\internship\\tvsm_speechbrain_explore\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install soundfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP-ICFndDDDV",
        "outputId": "7ebb9e82-e809-4607-f2a4-15c7fc1832cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arun vs Arun - tensor([True])\n"
          ]
        }
      ],
      "source": [
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "import torchaudio\n",
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "score, prediction = verification.verify_files(\"Sound Recordings/aruntestarticle.wav\", \"Sound Recordings/aruntestarticle.wav\") # Same Speakers\n",
        "print(f'Arun vs Arun - {prediction}')\n",
        "# score, prediction = verification.verify_files(\"/content/ArticleUnBroken/sunamdhatestarticle.wav\", \"/content/ArticleUnBroken/sunamdhatestarticle.wav\") # Same Speakers\n",
        "# print(f'Sunamdha vs Sunamdha - {prediction}')\n",
        "# score, prediction = verification.verify_files(\"/content/ArticleUnBroken/anirbantestarticle.wav\", \"/content/ArticleUnBroken/anirbantestarticle.wav\") # Same Speakers\n",
        "# print(f'Anirban vs Anirban - {prediction}')\n",
        "# score, prediction = verification.verify_files(\"/content/ArticleUnBroken/shivam_gmm.wav\", \"/content/ArticleUnBroken/shivam_gmm.wav\") # Same Speakers\n",
        "# print(f'Shivam vs Shivam - {prediction}')\n",
        "# score, prediction = verification.verify_files(\"/content/ArticleUnBroken/abhishektestarticle.wav\", \"/content/ArticleUnBroken/abhishektestarticle.wav\") # Same Speakers\n",
        "# print(f'Abhishek vs Abhishek - {prediction}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvQPVTOX9tae",
        "outputId": "00219222-deba-46a7-e78f-c543af61b7fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction for pair (aruntestarticle.wav, shivam_gmm.wav): tensor([True])\n",
            "Prediction for pair (aruntestarticle.wav, abhishektestarticle.wav): tensor([True])\n",
            "Prediction for pair (aruntestarticle.wav, sunamdhatestarticle.wav): tensor([False])\n",
            "Prediction for pair (aruntestarticle.wav, anirbantestarticle.wav): tensor([False])\n",
            "Prediction for pair (shivam_gmm.wav, abhishektestarticle.wav): tensor([False])\n",
            "Prediction for pair (shivam_gmm.wav, sunamdhatestarticle.wav): tensor([False])\n",
            "Prediction for pair (shivam_gmm.wav, anirbantestarticle.wav): tensor([False])\n",
            "Prediction for pair (abhishektestarticle.wav, sunamdhatestarticle.wav): tensor([False])\n",
            "Prediction for pair (abhishektestarticle.wav, anirbantestarticle.wav): tensor([True])\n",
            "Prediction for pair (sunamdhatestarticle.wav, anirbantestarticle.wav): tensor([False])\n"
          ]
        }
      ],
      "source": [
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "# Load the pre-trained speaker recognition model\n",
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "\n",
        "# Define the folder containing the audio files\n",
        "folder_path = \"Sound Recordings\" \n",
        "\n",
        "# List all the audio files in the folder\n",
        "audio_files = [\"aruntestarticle.wav\", \"shivam_gmm.wav\", \"abhishektestarticle.wav\", \"sunamdhatestarticle.wav\",\n",
        "               \"anirbantestarticle.wav\"]  # Add other files as needed\n",
        "\n",
        "# Generate all possible pairs of files\n",
        "file_pairs = list(itertools.combinations(audio_files, 2))\n",
        "\n",
        "# Iterate through each pair of files and get the predictions\n",
        "processed_pairs = set()  # To keep track of processed pairs\n",
        "\n",
        "for file1, file2 in file_pairs:\n",
        "    # Check if the pair has been processed in reverse order\n",
        "    if (file2, file1) in processed_pairs or (file1 == file2):\n",
        "        continue  # Skip if the reverse pair has already been processed or if it's the same speaker against themselves\n",
        "\n",
        "    # Get the full file paths\n",
        "    file1_path = os.path.join(folder_path, file1)\n",
        "    file2_path = os.path.join(folder_path, file2)\n",
        "\n",
        "    # Verify the speaker using the model\n",
        "    score, prediction = verification.verify_files(file1_path, file2_path)\n",
        "\n",
        "    # Print the prediction\n",
        "    print(f\"Prediction for pair ({file1}, {file2}): {prediction}\")\n",
        "\n",
        "    # Add the processed pair to the set\n",
        "    processed_pairs.add((file1, file2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "vNpRkL2jCAnQ",
        "outputId": "cb67178a-165f-42e1-e23b-e15f55cbafd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of files to compare from each folder: 5\n",
            "Accuracy for /content/audio_5sec/Abhishek * /content/audio_5sec/Abhishek = 64.0\n",
            "Accuracy for /content/audio_5sec/Abhishek * /content/audio_5sec/Sunamdha = 44.0\n",
            "Accuracy for /content/audio_5sec/Abhishek * /content/audio_5sec/Shivam = 52.0\n",
            "Accuracy for /content/audio_5sec/Abhishek * /content/audio_5sec/Arunanshu = 44.0\n",
            "Accuracy for /content/audio_5sec/Abhishek * /content/audio_5sec/Anirban = 44.0\n",
            "Accuracy for /content/audio_5sec/Sunamdha * /content/audio_5sec/Sunamdha = 52.0\n",
            "Accuracy for /content/audio_5sec/Sunamdha * /content/audio_5sec/Shivam = 52.0\n",
            "Accuracy for /content/audio_5sec/Sunamdha * /content/audio_5sec/Arunanshu = 44.0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a9b322ed06e2>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfile2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_folder2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# Verify the speaker using the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Check if the prediction is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/speaker.py\u001b[0m in \u001b[0;36mverify_files\u001b[0;34m(self, path_x, path_y, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaveform_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Verify:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Squeeze:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/speaker.py\u001b[0m in \u001b[0;36mverify_batch\u001b[0;34m(self, wavs1, wavs2, wav1_lens, wav2_lens, threshold)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[1;32m     88\u001b[0m         \u001b[0memb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav1_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0memb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav2_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/classifiers.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_var_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             embeddings = self.hparams.mean_var_norm_emb(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/lobes/models/ECAPA_TDNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# Multi-layer feature aggregation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# Attentive Statistical Pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/lobes/models/ECAPA_TDNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;34m\"\"\"Processes the input tensor x and returns an output tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/CNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    443\u001b[0m             )\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mwx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    304\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 306\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    307\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Load the pre-trained speaker recognition model\n",
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "\n",
        "# Define the folder names\n",
        "folder_names = [\"/content/audio_5sec/Abhishek\", \"/content/audio_5sec/Sunamdha\", \"/content/audio_5sec/Shivam\",\n",
        "                \"/content/audio_5sec/Arunanshu\", \"/content/audio_5sec/Anirban\"]\n",
        "\n",
        "# Number of files to compare from each folder\n",
        "num_files_to_compare = int(input(\"Enter the number of files to compare from each folder: \"))\n",
        "\n",
        "# Initialize a list to store results\n",
        "results = []\n",
        "\n",
        "# Loop through each pair of folders\n",
        "for i in range(len(folder_names)):\n",
        "    for j in range(i, len(folder_names)):\n",
        "        folder1 = folder_names[i]\n",
        "        folder2 = folder_names[j]\n",
        "\n",
        "        # Get the list of audio files in each folder\n",
        "        files_folder1 = random.sample(os.listdir(folder1), num_files_to_compare)\n",
        "        files_folder2 = random.sample(os.listdir(folder2), num_files_to_compare)\n",
        "\n",
        "        # Calculate the total number of comparisons for normalization\n",
        "        total_comparisons = num_files_to_compare * num_files_to_compare\n",
        "\n",
        "        # Initialize variables to track correct predictions\n",
        "        correct_predictions = 0\n",
        "\n",
        "        # Iterate through each file in both folders\n",
        "        for file1 in files_folder1:\n",
        "            for file2 in files_folder2:\n",
        "                # Verify the speaker using the model\n",
        "                score, prediction = verification.verify_files(os.path.join(folder1, file1), os.path.join(folder2, file2))\n",
        "\n",
        "                # Check if the prediction is correct\n",
        "                if prediction:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = (correct_predictions / total_comparisons) * 100\n",
        "\n",
        "        # Append results to the list\n",
        "        results.append({'Folder1': folder1, 'Folder2': folder2, 'Accuracy': accuracy})\n",
        "        print(f'Accuracy for {folder1} * {folder2} = {accuracy}')\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuPaQOHULspq",
        "outputId": "aec18d1b-94aa-4970-ffc5-68f9ed3cc7f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/audio/Abhishek/63.wav vs 160.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 356.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 266.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 284.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 291.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 48.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 298.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 314.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 230.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 126.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 77.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 246.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 136.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 190.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 303.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 87.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 3.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 30.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 155.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 138.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 179.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 335.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 55.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 68.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 256.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 162.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 297.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 185.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 213.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 312.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 271.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 172.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 149.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 254.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 89.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 223.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 122.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 208.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 218.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 235.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 120.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 251.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 363.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 358.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 13.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 21.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 62.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 276.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 90.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 50.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 304.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 244.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 156.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 121.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 171.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 58.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 296.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 35.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 71.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 154.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 224.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 348.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 123.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 42.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 268.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 328.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 92.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 333.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 181.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 180.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 153.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 78.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 317.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 326.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 334.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 109.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 323.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 325.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 16.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 270.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 6.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 231.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 46.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 115.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 209.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 238.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 119.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 11.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 44.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 141.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 159.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 20.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 221.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 347.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 164.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 282.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 26.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 263.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 33.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 301.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 94.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 261.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 234.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 318.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 310.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 357.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 12.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 56.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 74.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 202.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 47.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 293.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 287.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 320.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 108.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 307.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 300.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 17.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 84.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 214.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 170.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 366.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 306.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 70.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 337.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 262.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 127.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 116.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 146.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 128.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 139.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 95.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 38.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 278.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 82.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 161.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 100.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 118.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 23.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 14.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 332.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 319.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 283.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 350.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 22.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 299.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 103.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 220.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 327.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 346.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 2.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 359.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 107.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 158.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 168.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 177.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 101.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 279.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 37.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 4.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 133.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 189.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 360.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 27.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 253.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 227.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 60.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 219.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 152.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 86.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 165.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 96.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 248.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 73.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 104.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 117.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 206.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 81.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 98.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 174.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 226.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 182.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 364.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 285.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 43.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 294.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 52.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 295.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 18.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 114.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 344.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 290.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 184.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 342.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 142.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 49.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 201.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 143.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 135.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 198.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 157.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 167.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 72.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 324.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 176.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 129.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 145.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 302.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 137.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 330.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 163.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 241.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 367.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 361.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 75.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 67.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 225.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 69.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 354.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 245.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 57.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 205.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 260.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 169.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 54.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 134.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 252.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 255.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 305.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 15.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 352.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 124.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 151.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 24.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 243.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 336.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 144.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 233.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 341.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 99.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 183.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 10.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 273.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 199.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 173.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 8.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 97.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 53.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 329.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 267.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 83.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 112.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 105.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 229.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 140.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 1.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 85.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 79.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 147.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 258.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 222.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 275.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 193.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 192.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 365.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 217.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 9.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 308.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 88.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 249.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 65.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 41.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 175.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 216.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 36.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 239.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 76.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 247.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 132.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 349.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 345.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 281.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 61.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 242.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 66.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 277.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 343.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 286.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 315.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 210.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 59.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 355.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 194.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 195.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 186.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 232.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 200.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 63.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 240.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 148.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 111.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 7.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 196.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 289.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 264.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 331.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 106.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 25.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 93.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 130.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 178.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 91.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 316.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 131.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 102.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 39.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 257.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 351.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 29.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 19.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 368.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 191.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 265.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 311.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 110.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 188.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 274.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 236.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 34.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 237.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 353.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 259.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 187.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 250.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 215.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 362.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 150.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 338.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 113.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 125.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 31.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 340.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 272.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 203.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 197.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 269.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 166.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 204.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 228.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 211.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 45.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 309.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 212.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 280.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 322.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 40.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 5.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 64.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 32.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 288.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 313.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 80.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 339.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 28.wav - prediction: tensor([False])\n",
            "/content/audio/Abhishek/63.wav vs 321.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 207.wav - prediction: tensor([True])\n",
            "/content/audio/Abhishek/63.wav vs 51.wav - prediction: tensor([True])\n",
            "Accuracy for /content/audio/Abhishek * /content/audio/Sunamdha = 72.47956403269755\n",
            "                   Folder1                  Folder2   Accuracy\n",
            "0  /content/audio/Abhishek  /content/audio/Sunamdha  72.479564\n"
          ]
        }
      ],
      "source": [
        "# one vs many -\n",
        "import pandas as pd\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Load the pre-trained speaker recognition model\n",
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "\n",
        "# Define the folder names\n",
        "folder_names = [\"/content/audio_5sec/Abhishek\", \"/content/audio_5sec/Sunamdha\"]\n",
        "\n",
        "# Number of files to compare from each folder\n",
        "num_files_to_compare = 367\n",
        "\n",
        "# Initialize a list to store results\n",
        "results = []\n",
        "\n",
        "# Loop through each pair of folders\n",
        "for i in range(len(folder_names)):\n",
        "    for j in range(i+1, len(folder_names)):\n",
        "        folder1 = folder_names[i]\n",
        "        folder2 = folder_names[j]\n",
        "\n",
        "        # Get the list of audio files in each folder\n",
        "        files_folder1 = [\"/content/audio_5sec/Abhishek/63.wav\"]\n",
        "        files_folder2 = random.sample(os.listdir(folder2), num_files_to_compare)\n",
        "\n",
        "        # Calculate the total number of comparisons for normalization\n",
        "        total_comparisons = 1 * num_files_to_compare\n",
        "\n",
        "        # Initialize variables to track correct predictions\n",
        "        correct_predictions = 0\n",
        "\n",
        "        # Iterate through each file in both folders\n",
        "        for file1 in files_folder1:\n",
        "            for file2 in files_folder2:\n",
        "                # Verify the speaker using the model\n",
        "                score, prediction = verification.verify_files(os.path.join(folder1, file1), os.path.join(folder2, file2))\n",
        "                print(f'{file1} vs {file2} - prediction: {prediction}')\n",
        "                # # Check if the prediction is correct\n",
        "                if prediction:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = (correct_predictions / total_comparisons) * 100\n",
        "\n",
        "        # Append results to the list\n",
        "        results.append({'Folder1': folder1, 'Folder2': folder2, 'Accuracy': accuracy})\n",
        "        print(f'Accuracy for {folder1} * {folder2} = {accuracy}')\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "8Oo_xV3t-pqg",
        "outputId": "cf2bf507-4967-4c75-a004-1bab759c171f"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7d2814884100>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfile2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_folder2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# Verify the speaker using the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# Check if the prediction is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/speaker.py\u001b[0m in \u001b[0;36mverify_files\u001b[0;34m(self, path_x, path_y, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaveform_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Verify:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Squeeze:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/speaker.py\u001b[0m in \u001b[0;36mverify_batch\u001b[0;34m(self, wavs1, wavs2, wav1_lens, wav2_lens, threshold)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mspeaker\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0memb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav1_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0memb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav2_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/classifiers.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_var_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             embeddings = self.hparams.mean_var_norm_emb(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/lobes/models/ECAPA_TDNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# Multi-layer feature aggregation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# Attentive Statistical Pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/lobes/models/ECAPA_TDNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;34m\"\"\"Processes the input tensor x and returns an output tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/CNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    443\u001b[0m             )\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mwx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    304\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 306\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    307\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "import os\n",
        "\n",
        "# Load the pre-trained speaker recognition model\n",
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "\n",
        "# Define the folder names\n",
        "folder_names = [\"/content/audio/Abhishek\", \"/content/audio/Anirban\", \"/content/audio/Arunanshu\",\n",
        "                \"/content/audio/Shivam\", \"/content/audio/Sunamdha\"]\n",
        "\n",
        "# Initialize a list to store results\n",
        "results = []\n",
        "\n",
        "# Loop through each pair of folders\n",
        "for i in range(len(folder_names)):\n",
        "    for j in range(i+1, len(folder_names)):\n",
        "        folder1 = folder_names[i]\n",
        "        folder2 = folder_names[j]\n",
        "\n",
        "        # Get the list of audio files in each folder\n",
        "        files_folder1 = os.listdir(folder1)\n",
        "        files_folder2 = os.listdir(folder2)\n",
        "\n",
        "        # Calculate the total number of comparisons for normalization\n",
        "        total_comparisons = len(files_folder1) * len(files_folder2)\n",
        "\n",
        "        # Initialize variables to track correct predictions\n",
        "        correct_predictions = 0\n",
        "\n",
        "        # Iterate through each file in both folders\n",
        "        for file1 in files_folder1:\n",
        "            for file2 in files_folder2:\n",
        "                # Verify the speaker using the model\n",
        "                score, prediction = verification.verify_files(os.path.join(folder1, file1), os.path.join(folder2, file2))\n",
        "\n",
        "                # Check if the prediction is correct\n",
        "                if prediction:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / total_comparisons * 100\n",
        "\n",
        "        # Append results to the list\n",
        "        results.append({'Folder1': folder1, 'Folder2': folder2, 'Accuracy': accuracy})\n",
        "        print(f'Accuracy for {folder1} * {folder2} = {accuracy}')\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Print the DataFrame\n",
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recording audio...\n",
            "Audio recording complete.\n",
            "Saving audio...\n",
            "Audio saved as unknown_voice.wav\n",
            "tensor([0.9242])\n",
            "Both users are same\n"
          ]
        }
      ],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "\n",
        "def record_and_save_audio(filename, duration=60, sample_rate=44100, channels=2):\n",
        "    print(\"Recording audio...\")\n",
        "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=channels, dtype=np.int16)\n",
        "    sd.wait()\n",
        "    print(\"Audio recording complete.\")\n",
        "\n",
        "    print(\"Saving audio...\")\n",
        "    wav.write(filename, sample_rate, audio_data)\n",
        "    print(f\"Audio saved as {filename}\")\n",
        "\n",
        "\n",
        "filename = \"unknown_voice.wav\"\n",
        "record_and_save_audio(filename)\n",
        "\n",
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "score, prediction = verification.verify_files(\"arun_test.wav\",\n",
        "                                               \"unknown_voice.wav\") # Same Speaker\n",
        "print(score)\n",
        "if score > 0.5:\n",
        "    print(\"Both users are same\")\n",
        "else:\n",
        "    print(\"Who are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "C:/Users/Dell/Desktop/EDU/Internship/TVSM_SpeechBrain_Explore/SpeechBrainExplore/Sound Recordings/Anirban3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.3857])\n",
            "Who are you?\n"
          ]
        }
      ],
      "source": [
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "score, prediction = verification.verify_files(\"arun_test.wav\",\n",
        "                                               \"abhishek_test.wav\") # Same Speaker\n",
        "print(score)\n",
        "if score > 0.5:\n",
        "    print(\"Both users are same\")\n",
        "else:\n",
        "    print(\"Who are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
