{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 'prononciation' files\n",
    "# generate the df with mfcc, vector, row mean\n",
    "# train all models from additional_features file\n",
    "# collect 5 test samples\n",
    "# augment to 5*7 = 35\n",
    "# predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronounciation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['augmented_audio/Abhishek10Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek10Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek10Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek10Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek10Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek10Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek10Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Abhishek1Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek1Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek1Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek1Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek1Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek1Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek1Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Abhishek2Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek2Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek2Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek2Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek2Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek2Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek2Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Abhishek3Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek3Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek3Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek3Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek3Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek3Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek3Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Abhishek4Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek4Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek4Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek4Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek4Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek4Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek4Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Abhishek5Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek5Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek5Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek5Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek5Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek5Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek5Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Abhishek6Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek6Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek6Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek6Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek6Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek6Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek6Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Abhishek7Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek7Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek7Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek7Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek7Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek7Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek7Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Abhishek8Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek8Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek8Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek8Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek8Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek8Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek8Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Abhishek9Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Abhishek9Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Abhishek9Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Abhishek9Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Abhishek9Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Abhishek9Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Abhishek9Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun10Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun10Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun10Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun10Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun10Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun10Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun10Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun1Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun1Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun1Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun1Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun1Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun1Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun1Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun2Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun2Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun2Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun2Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun2Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun2Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun2Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun3Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun3Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun3Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun3Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun3Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun3Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun3Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun4Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun4Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun4Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun4Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun4Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun4Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun4Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun5Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun5Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun5Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun5Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun5Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun5Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun5Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun6Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun6Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun6Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun6Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun6Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun6Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun6Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun7Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun7Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun7Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun7Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun7Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun7Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun7Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun8Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun8Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun8Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun8Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun8Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun8Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun8Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Arun9Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Arun9Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Arun9Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Arun9Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Arun9Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Arun9Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Arun9Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha10Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha10Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha10Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha10Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha10Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha10Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha10Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha1Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha1Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha1Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha1Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha1Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha1Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha1Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha2Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha2Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha2Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha2Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha2Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha2Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha2Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha3Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha3Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha3Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha3Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha3Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha3Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha3Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha4Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha4Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha4Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha4Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha4Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha4Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha4Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha5Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha5Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha5Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha5Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha5Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha5Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha5Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha6Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha6Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha6Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha6Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha6Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha6Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha6Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha7Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha7Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha7Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha7Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha7Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha7Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha7Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha8Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha8Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha8Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha8Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha8Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha8Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha8Pronounciation.wavoutput_stretched.wav',\n",
       " 'augmented_audio/Sunamdha9Pronounciation.wavoutput_augmented.wav',\n",
       " 'augmented_audio/Sunamdha9Pronounciation.wavoutput_compressed.wav',\n",
       " 'augmented_audio/Sunamdha9Pronounciation.wavoutput_cropped.wav',\n",
       " 'augmented_audio/Sunamdha9Pronounciation.wavoutput_noisy.wav',\n",
       " 'augmented_audio/Sunamdha9Pronounciation.wavoutput_pitch_shifted.wav',\n",
       " 'augmented_audio/Sunamdha9Pronounciation.wavoutput_speed_changed.wav',\n",
       " 'augmented_audio/Sunamdha9Pronounciation.wavoutput_stretched.wav']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = []\n",
    "for filename in os.listdir(\"augmented_audio/\"):\n",
    "    if \"pronounciation\" in filename.lower():\n",
    "        files.append(\"augmented_audio/\" + filename)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating df of Pronounciation files - MFCC, Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\govindarajula\\AppData\\Local\\Temp\\ipykernel_16480\\443737682.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word: Pronounciation\n",
      "         Name                                               MFCC  \\\n",
      "0    Abhishek  [-399.33517, -410.98068, -446.12094, -462.5722...   \n",
      "1    Abhishek  [-393.50916, -416.78497, -443.1696, -447.20905...   \n",
      "2    Abhishek  [-398.04355, -407.43915, -423.16266, -427.1824...   \n",
      "3    Abhishek  [-279.38562, -255.72926, -259.68298, -265.5101...   \n",
      "4    Abhishek  [-398.58353, -407.81512, -439.38373, -454.3276...   \n",
      "..        ...                                                ...   \n",
      "205  Sunamdha  [-454.1231, -468.5504, -485.27255, -494.31076,...   \n",
      "206  Sunamdha  [-304.044, -272.84253, -268.24622, -271.28018,...   \n",
      "207  Sunamdha  [-455.58807, -471.42218, -504.5795, -518.75836...   \n",
      "208  Sunamdha  [-445.229, -472.5361, -499.69522, -508.4312, -...   \n",
      "209  Sunamdha  [-453.48724, -471.49097, -492.32755, -502.4375...   \n",
      "\n",
      "                                                Vector  \n",
      "0    [1.4150455334195575, 0.9546512831353698, 0.366...  \n",
      "1    [0.747931590109873, 0.4648794312754385, 0.2183...  \n",
      "2    [0.35104955499818213, 0.8060239526847797, 0.11...  \n",
      "3    [0.008702108523313145, 0.83070381055796, 1.778...  \n",
      "4    [0.9068272101350625, 0.8688605322756997, 0.287...  \n",
      "..                                                 ...  \n",
      "205  [-0.5167410570874565, -0.4283380087267415, 0.3...  \n",
      "206  [-0.41733527759116845, -0.6118640605585646, -0...  \n",
      "207  [-0.20458474852108818, -0.3249366339081963, 0....  \n",
      "208  [-0.47416335629183765, -0.5520375781211978, 0....  \n",
      "209  [-0.207728415523001, -0.4549384943630632, 0.34...  \n",
      "\n",
      "[210 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>MFCC</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>[-399.33517, -410.98068, -446.12094, -462.5722...</td>\n",
       "      <td>[1.4150455334195575, 0.9546512831353698, 0.366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>[-393.50916, -416.78497, -443.1696, -447.20905...</td>\n",
       "      <td>[0.747931590109873, 0.4648794312754385, 0.2183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>[-398.04355, -407.43915, -423.16266, -427.1824...</td>\n",
       "      <td>[0.35104955499818213, 0.8060239526847797, 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>[-279.38562, -255.72926, -259.68298, -265.5101...</td>\n",
       "      <td>[0.008702108523313145, 0.83070381055796, 1.778...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>[-398.58353, -407.81512, -439.38373, -454.3276...</td>\n",
       "      <td>[0.9068272101350625, 0.8688605322756997, 0.287...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>[-454.1231, -468.5504, -485.27255, -494.31076,...</td>\n",
       "      <td>[-0.5167410570874565, -0.4283380087267415, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>[-304.044, -272.84253, -268.24622, -271.28018,...</td>\n",
       "      <td>[-0.41733527759116845, -0.6118640605585646, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>[-455.58807, -471.42218, -504.5795, -518.75836...</td>\n",
       "      <td>[-0.20458474852108818, -0.3249366339081963, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>[-445.229, -472.5361, -499.69522, -508.4312, -...</td>\n",
       "      <td>[-0.47416335629183765, -0.5520375781211978, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>[-453.48724, -471.49097, -492.32755, -502.4375...</td>\n",
       "      <td>[-0.207728415523001, -0.4549384943630632, 0.34...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name                                               MFCC  \\\n",
       "0    Abhishek  [-399.33517, -410.98068, -446.12094, -462.5722...   \n",
       "1    Abhishek  [-393.50916, -416.78497, -443.1696, -447.20905...   \n",
       "2    Abhishek  [-398.04355, -407.43915, -423.16266, -427.1824...   \n",
       "3    Abhishek  [-279.38562, -255.72926, -259.68298, -265.5101...   \n",
       "4    Abhishek  [-398.58353, -407.81512, -439.38373, -454.3276...   \n",
       "..        ...                                                ...   \n",
       "205  Sunamdha  [-454.1231, -468.5504, -485.27255, -494.31076,...   \n",
       "206  Sunamdha  [-304.044, -272.84253, -268.24622, -271.28018,...   \n",
       "207  Sunamdha  [-455.58807, -471.42218, -504.5795, -518.75836...   \n",
       "208  Sunamdha  [-445.229, -472.5361, -499.69522, -508.4312, -...   \n",
       "209  Sunamdha  [-453.48724, -471.49097, -492.32755, -502.4375...   \n",
       "\n",
       "                                                Vector  \n",
       "0    [1.4150455334195575, 0.9546512831353698, 0.366...  \n",
       "1    [0.747931590109873, 0.4648794312754385, 0.2183...  \n",
       "2    [0.35104955499818213, 0.8060239526847797, 0.11...  \n",
       "3    [0.008702108523313145, 0.83070381055796, 1.778...  \n",
       "4    [0.9068272101350625, 0.8688605322756997, 0.287...  \n",
       "..                                                 ...  \n",
       "205  [-0.5167410570874565, -0.4283380087267415, 0.3...  \n",
       "206  [-0.41733527759116845, -0.6118640605585646, -0...  \n",
       "207  [-0.20458474852108818, -0.3249366339081963, 0....  \n",
       "208  [-0.47416335629183765, -0.5520375781211978, 0....  \n",
       "209  [-0.207728415523001, -0.4549384943630632, 0.34...  \n",
       "\n",
       "[210 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn import preprocessing\n",
    "import python_speech_features as mfcc\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=25):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfccs.flatten()\n",
    "\n",
    "# Function to extract features (MFCC and delta coefficients)\n",
    "def extract_features(audio, rate):\n",
    "    mfcc_feature = mfcc.mfcc(audio, rate, 0.025, 0.01, 20, nfft=1200, appendEnergy=True)\n",
    "    mfcc_feature = preprocessing.scale(mfcc_feature)\n",
    "    delta = calculate_delta(mfcc_feature)\n",
    "    combined = np.hstack((mfcc_feature, delta))\n",
    "    return combined.flatten()\n",
    "\n",
    "# Function to calculate delta coefficients\n",
    "def calculate_delta(array):\n",
    "    rows, cols = array.shape\n",
    "    deltas = np.zeros((rows, 20))\n",
    "    n = 2\n",
    "    for i in range(rows):\n",
    "        index = []\n",
    "        j = 1\n",
    "        while j <= n:\n",
    "            if i - j < 0:\n",
    "                first = 0\n",
    "            else:\n",
    "                first = i - j\n",
    "            if i + j > rows - 1:\n",
    "                second = rows - 1\n",
    "            else:\n",
    "                second = i + j\n",
    "            index.append((second, first))\n",
    "            j += 1\n",
    "        deltas[i] = (array[index[0][0]] - array[index[0][1]] + (2 * (array[index[1][0]] - array[index[1][1]]))) / 10\n",
    "    return deltas\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = \"augmented_audio/\"\n",
    "\n",
    "# List all files in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Create a dictionary to store data for each word\n",
    "word_data = {}\n",
    "\n",
    "# Traverse through each file\n",
    "for file_name in files:\n",
    "    if \"pronounciation\" in file_name.lower():\n",
    "        if file_name.lower().endswith(\".wav\"):\n",
    "            # Parse the file name to extract information\n",
    "            pattern = r'([A-Za-z]+)\\d+([A-Za-z]+)'\n",
    "            # Use re.match to find the pattern in the file name\n",
    "            match = re.match(pattern, file_name)\n",
    "            if match:\n",
    "                # Extract the name and word from the matched groups\n",
    "                name = match.group(1)\n",
    "                word = match.group(2)\n",
    "            \n",
    "            # Check if the word is already in the dictionary\n",
    "            if word not in word_data:\n",
    "                word_data[word] = {'Name': [], 'MFCC': [], 'Vector': []}\n",
    "\n",
    "            # Load the original audio and extract MFCC\n",
    "            input_file_path = os.path.join(folder_path, file_name)\n",
    "            mfccs = extract_mfcc(input_file_path)\n",
    "            \n",
    "            # Load the original audio\n",
    "            sr, audio = read(os.path.join(folder_path, file_name))\n",
    "            \n",
    "            # Extract features (MFCC and delta coefficients)\n",
    "            features = extract_features(audio, sr)\n",
    "            \n",
    "            # Add data to the dictionary\n",
    "            word_data[word]['Name'].append(name)\n",
    "            word_data[word]['MFCC'].append(mfccs)\n",
    "            word_data[word]['Vector'].append(features)  # Add the vector values\n",
    "\n",
    "# Create DataFrames for each word\n",
    "word_dfs = {}\n",
    "for word, data in word_data.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    word_dfs[word] = df\n",
    "\n",
    "# Display DataFrames for each word\n",
    "for word, df in word_dfs.items():\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training all models on MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\AdjustedPythonVirtEnv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[12  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 15]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      1.00      1.00        12\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "\n",
      "Training and evaluating Decision Tree...\n",
      "Accuracy: 0.90\n",
      "Confusion Matrix:\n",
      "[[11  0  1]\n",
      " [ 2 13  0]\n",
      " [ 0  1 14]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.85      0.92      0.88        12\n",
      "        Arun       0.93      0.87      0.90        15\n",
      "    Sunamdha       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.90      0.91      0.90        42\n",
      "weighted avg       0.91      0.90      0.90        42\n",
      "\n",
      "\n",
      "Training and evaluating Random Forest...\n",
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[12  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 15]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      1.00      1.00        12\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Accuracy: 0.95\n",
      "Confusion Matrix:\n",
      "[[12  0  0]\n",
      " [ 0 15  0]\n",
      " [ 2  0 13]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.86      1.00      0.92        12\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.95        42\n",
      "   macro avg       0.95      0.96      0.95        42\n",
      "weighted avg       0.96      0.95      0.95        42\n",
      "\n",
      "\n",
      "Training and evaluating Support Vector Machine...\n",
      "Accuracy: 0.64\n",
      "Confusion Matrix:\n",
      "[[ 8  1  3]\n",
      " [ 0  8  7]\n",
      " [ 1  3 11]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.89      0.67      0.76        12\n",
      "        Arun       0.67      0.53      0.59        15\n",
      "    Sunamdha       0.52      0.73      0.61        15\n",
      "\n",
      "    accuracy                           0.64        42\n",
      "   macro avg       0.69      0.64      0.66        42\n",
      "weighted avg       0.68      0.64      0.65        42\n",
      "\n",
      "\n",
      "Training and evaluating K-Nearest Neighbors...\n",
      "Accuracy: 0.93\n",
      "Confusion Matrix:\n",
      "[[11  0  1]\n",
      " [ 0 15  0]\n",
      " [ 2  0 13]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.85      0.92      0.88        12\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       0.93      0.87      0.90        15\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.92      0.93      0.93        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n",
      "\n",
      "Training and evaluating Naive Bayes...\n",
      "Accuracy: 0.81\n",
      "Confusion Matrix:\n",
      "[[10  0  2]\n",
      " [ 0 11  4]\n",
      " [ 2  0 13]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.83      0.83      0.83        12\n",
      "        Arun       1.00      0.73      0.85        15\n",
      "    Sunamdha       0.68      0.87      0.76        15\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.84      0.81      0.81        42\n",
      "weighted avg       0.84      0.81      0.81        42\n",
      "\n",
      "\n",
      "Training and evaluating MLP Classifier...\n",
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[12  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 15]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      1.00      1.00        12\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "\n",
      "Training and evaluating AdaBoost Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AdjustedPythonVirtEnv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n",
      "Confusion Matrix:\n",
      "[[ 1  0 11]\n",
      " [ 2 11  2]\n",
      " [ 0  0 15]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.33      0.08      0.13        12\n",
      "        Arun       1.00      0.73      0.85        15\n",
      "    Sunamdha       0.54      1.00      0.70        15\n",
      "\n",
      "    accuracy                           0.64        42\n",
      "   macro avg       0.62      0.61      0.56        42\n",
      "weighted avg       0.64      0.64      0.59        42\n",
      "\n",
      "\n",
      "Training and evaluating Bagging Classifier...\n",
      "Accuracy: 0.86\n",
      "Confusion Matrix:\n",
      "[[11  1  0]\n",
      " [ 2 13  0]\n",
      " [ 2  1 12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.73      0.92      0.81        12\n",
      "        Arun       0.87      0.87      0.87        15\n",
      "    Sunamdha       1.00      0.80      0.89        15\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.87      0.86      0.86        42\n",
      "weighted avg       0.88      0.86      0.86        42\n",
      "\n",
      "\n",
      "Training and evaluating Extra Trees Classifier...\n",
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[12  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 15]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      1.00      1.00        12\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "\n",
      "Training and evaluating Quadratic Discriminant Analysis...\n",
      "Accuracy: 0.64\n",
      "Confusion Matrix:\n",
      "[[10  0  2]\n",
      " [ 2  7  6]\n",
      " [ 4  1 10]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.62      0.83      0.71        12\n",
      "        Arun       0.88      0.47      0.61        15\n",
      "    Sunamdha       0.56      0.67      0.61        15\n",
      "\n",
      "    accuracy                           0.64        42\n",
      "   macro avg       0.69      0.66      0.64        42\n",
      "weighted avg       0.69      0.64      0.64        42\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AdjustedPythonVirtEnv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "df_vector = df.copy()\n",
    "\n",
    "# Assuming df is your DataFrame with 'Vector' and 'Label' columns\n",
    "X = df_vector['MFCC'].values\n",
    "y = df_vector['Name']\n",
    "\n",
    "# Define a custom padding function\n",
    "def pad_sequences_with_mean(sequences, max_length):\n",
    "    padded_sequences = np.zeros((len(sequences), max_length))\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        if seq_len > 0:\n",
    "            mean_value = np.mean(seq)\n",
    "            padded_sequences[i, :seq_len] = seq\n",
    "            padded_sequences[i, seq_len:] = mean_value\n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "# Find the maximum length of sequences\n",
    "max_length = max(len(seq) for seq in X)\n",
    "\n",
    "# Pad the sequences with the mean value\n",
    "X_padded = pad_sequences_with_mean(X, max_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize various classifiers and create a dictionary to store trained models\n",
    "trained_models = {}\n",
    "\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'MLP Classifier': MLPClassifier(),\n",
    "    'AdaBoost Classifier': AdaBoostClassifier(),\n",
    "    'Bagging Classifier': BaggingClassifier(),\n",
    "    'Extra Trees Classifier': ExtraTreesClassifier(),\n",
    "    'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"\\nTraining and evaluating {name}...\")\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Save the trained model in the dictionary\n",
    "    trained_models[name] = classifier\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4325"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': LogisticRegression(),\n",
       " 'Decision Tree': DecisionTreeClassifier(),\n",
       " 'Random Forest': RandomForestClassifier(),\n",
       " 'Gradient Boosting': GradientBoostingClassifier(),\n",
       " 'Support Vector Machine': SVC(),\n",
       " 'K-Nearest Neighbors': KNeighborsClassifier(),\n",
       " 'Naive Bayes': GaussianNB(),\n",
       " 'MLP Classifier': MLPClassifier(),\n",
       " 'AdaBoost Classifier': AdaBoostClassifier(),\n",
       " 'Bagging Classifier': BaggingClassifier(),\n",
       " 'Extra Trees Classifier': ExtraTreesClassifier(),\n",
       " 'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis()}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect test sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording file audio_files/test/test/Sunamdha1Pronounciation.wav\n",
      "Pronounciation - 1\n",
      "Recorded file audio_files/test/test/Sunamdha1Pronounciation.wav\n",
      "Recording file audio_files/test/test/Sunamdha2Pronounciation.wav\n",
      "Pronounciation - 2\n",
      "Recorded file audio_files/test/test/Sunamdha2Pronounciation.wav\n",
      "Recording file audio_files/test/test/Sunamdha3Pronounciation.wav\n",
      "Pronounciation - 3\n",
      "Recorded file audio_files/test/test/Sunamdha3Pronounciation.wav\n",
      "Recording file audio_files/test/test/Sunamdha4Pronounciation.wav\n",
      "Pronounciation - 4\n",
      "Recorded file audio_files/test/test/Sunamdha4Pronounciation.wav\n",
      "Recording file audio_files/test/test/Sunamdha5Pronounciation.wav\n",
      "Pronounciation - 5\n",
      "Recorded file audio_files/test/test/Sunamdha5Pronounciation.wav\n",
      "Recording file audio_files/test/test/Sunamdha6Pronounciation.wav\n",
      "Pronounciation - 6\n",
      "Recorded file audio_files/test/test/Sunamdha6Pronounciation.wav\n",
      "Recording file audio_files/test/test/Sunamdha7Pronounciation.wav\n",
      "Pronounciation - 7\n",
      "Recorded file audio_files/test/test/Sunamdha7Pronounciation.wav\n",
      "Recording file audio_files/test/test/Sunamdha8Pronounciation.wav\n",
      "Pronounciation - 8\n",
      "Recorded file audio_files/test/test/Sunamdha8Pronounciation.wav\n",
      "Recording file audio_files/test/test/Sunamdha9Pronounciation.wav\n",
      "Pronounciation - 9\n",
      "Recorded file audio_files/test/test/Sunamdha9Pronounciation.wav\n",
      "Recording file audio_files/test/test/Sunamdha10Pronounciation.wav\n",
      "Pronounciation - 10\n",
      "Recorded file audio_files/test/test/Sunamdha10Pronounciation.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import wavio as wv\n",
    "\n",
    "name = \"Sunamdha\"\n",
    "freq = 44100\n",
    "duration = 2\n",
    "\n",
    "words = ['Pronounciation'] \n",
    "\n",
    "for i in range(10):\n",
    "    file_name = \"audio_files/test/test/\" + name + str(i+1) + word + '.wav'\n",
    "    print(\"Recording file \" + file_name)\n",
    "    print(f'{word} - {i+1}')\n",
    "    recording = sd.rec(int(duration * freq), samplerate=freq, channels=1)\n",
    "    sd.wait()\n",
    "    write(file_name, freq, recording)\n",
    "    print(\"Recorded file \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict live audio clip with all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn import preprocessing\n",
    "import python_speech_features as mfcc\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=25):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfccs.flatten()\n",
    "\n",
    "# Function to extract features (MFCC and delta coefficients)\n",
    "def extract_features(audio, rate):\n",
    "    mfcc_feature = mfcc.mfcc(audio, rate, 0.025, 0.01, 20, nfft=1200, appendEnergy=True)\n",
    "    mfcc_feature = preprocessing.scale(mfcc_feature)\n",
    "    delta = calculate_delta(mfcc_feature)\n",
    "    combined = np.hstack((mfcc_feature, delta))\n",
    "    return combined.flatten()\n",
    "\n",
    "# Function to calculate delta coefficients\n",
    "def calculate_delta(array):\n",
    "    rows, cols = array.shape\n",
    "    deltas = np.zeros((rows, 20))\n",
    "    n = 2\n",
    "    for i in range(rows):\n",
    "        index = []\n",
    "        j = 1\n",
    "        while j <= n:\n",
    "            if i - j < 0:\n",
    "                first = 0\n",
    "            else:\n",
    "                first = i - j\n",
    "            if i + j > rows - 1:\n",
    "                second = rows - 1\n",
    "            else:\n",
    "                second = i + j\n",
    "            index.append((second, first))\n",
    "            j += 1\n",
    "        deltas[i] = (array[index[0][0]] - array[index[0][1]] + (2 * (array[index[1][0]] - array[index[1][1]]))) / 10\n",
    "    return deltas\n",
    "\n",
    "def create_df_test_vec(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    # vector_data = {'Vector': []}\n",
    "    mfcc_data = {'MFCC': []}\n",
    "\n",
    "    for file_name in files:\n",
    "        if file_name.lower().endswith(\".wav\"):\n",
    "            # MFCC Dataset\n",
    "            input_file_path = os.path.join(folder_path, file_name)\n",
    "            mfccs = extract_mfcc(input_file_path)\n",
    "            mfcc_data['MFCC'].append(mfccs)\n",
    "\n",
    "            # Vector Dataset\n",
    "            # Load the original audio\n",
    "            # sr, audio = read(os.path.join(folder_path, file_name))\n",
    "            # # Extract features (MFCC and delta coefficients)\n",
    "            # features = extract_features(audio, sr)\n",
    "            # # Add vector to the dictionary\n",
    "            # vector_data['Vector'].append(features)  # Add the vector values\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_test_mfcc = pd.DataFrame(mfcc_data)\n",
    "\n",
    "    return df_test_mfcc\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = \"audio_files/test/\"\n",
    "\n",
    "# Create DataFrame\n",
    "df_test_mfcc = create_df_test_vec(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-498.53348, -490.7242, -496.195, -500.975, -5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-267.81674, -282.32547, -305.14893, -304.0901...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-238.04095, -300.84586, -435.73254, -435.6609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-485.6647, -365.62228, -308.145, -325.15506, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-502.50278, -481.8657, -482.20917, -483.40936...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                MFCC\n",
       "0  [-498.53348, -490.7242, -496.195, -500.975, -5...\n",
       "1  [-267.81674, -282.32547, -305.14893, -304.0901...\n",
       "2  [-238.04095, -300.84586, -435.73254, -435.6609...\n",
       "3  [-485.6647, -365.62228, -308.145, -325.15506, ...\n",
       "4  [-502.50278, -481.8657, -482.20917, -483.40936..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4325,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_mfcc['MFCC'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4325"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec = df_test_mfcc['MFCC'].values\n",
    "\n",
    "max_length_test = max(len(seq) for seq in X_test_vec)\n",
    "max_length_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <th>K-Nearest Neighbors</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>MLP Classifier</th>\n",
       "      <th>AdaBoost Classifier</th>\n",
       "      <th>Bagging Classifier</th>\n",
       "      <th>Extra Trees Classifier</th>\n",
       "      <th>Quadratic Discriminant Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Arun</td>\n",
       "      <td>Arun</td>\n",
       "      <td>Arun</td>\n",
       "      <td>Arun</td>\n",
       "      <td>Arun</td>\n",
       "      <td>Arun</td>\n",
       "      <td>Arun</td>\n",
       "      <td>Arun</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Abhishek</td>\n",
       "      <td>Sunamdha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Logistic Regression Decision Tree Random Forest Gradient Boosting  \\\n",
       "0            Abhishek      Sunamdha      Abhishek          Abhishek   \n",
       "1            Abhishek      Abhishek      Abhishek          Abhishek   \n",
       "2            Abhishek          Arun          Arun              Arun   \n",
       "3            Abhishek      Sunamdha      Abhishek          Abhishek   \n",
       "4            Abhishek      Abhishek      Abhishek          Abhishek   \n",
       "\n",
       "  Support Vector Machine K-Nearest Neighbors Naive Bayes MLP Classifier  \\\n",
       "0               Abhishek            Abhishek    Abhishek       Abhishek   \n",
       "1               Abhishek            Abhishek    Abhishek       Abhishek   \n",
       "2                   Arun                Arun        Arun           Arun   \n",
       "3               Sunamdha            Sunamdha    Abhishek       Abhishek   \n",
       "4               Sunamdha            Abhishek    Abhishek       Abhishek   \n",
       "\n",
       "  AdaBoost Classifier Bagging Classifier Extra Trees Classifier  \\\n",
       "0            Sunamdha           Abhishek               Abhishek   \n",
       "1            Sunamdha           Abhishek               Abhishek   \n",
       "2                Arun           Abhishek               Abhishek   \n",
       "3            Sunamdha           Sunamdha               Abhishek   \n",
       "4            Sunamdha           Abhishek               Abhishek   \n",
       "\n",
       "  Quadratic Discriminant Analysis  \n",
       "0                        Sunamdha  \n",
       "1                        Abhishek  \n",
       "2                        Abhishek  \n",
       "3                        Sunamdha  \n",
       "4                        Sunamdha  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assuming df_test_vec is your DataFrame with the 'Vector' column\n",
    "X_test_vec = df_test_mfcc['MFCC'].values\n",
    "\n",
    "# Find the maximum length of sequences for the test data\n",
    "max_length_test = max(len(seq) for seq in X_test_vec)\n",
    "\n",
    "# Pad or truncate the test sequences to match the training data's max_length\n",
    "X_test_padded = pad_sequences_with_mean(X_test_vec, max_length_test)\n",
    "\n",
    "# Initialize a dictionary to store predictions\n",
    "predictions = {}\n",
    "\n",
    "# Make predictions using the trained models\n",
    "for name, model in trained_models.items():\n",
    "    # Ensure the test_vector has the same shape as the training data\n",
    "    # For example, you may need to pad or reshape it\n",
    "    # test_vector_padded = pad_sequences_with_mean(X_test_vec, max_length_test)\n",
    "    \n",
    "    # Make the prediction\n",
    "    y_pred = model.predict(X_test_padded)\n",
    "    \n",
    "    # Store the predictions in the dictionary\n",
    "    predictions[name] = y_pred\n",
    "\n",
    "# Create a DataFrame from the predictions\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Display the predicted usernames\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best models dictionary - \n",
    "seleted_models = {\n",
    "    \"Arun\": [\"Decision Tree\", \"Random Forest\", \"Gradient Boosting\", \"Naive Bayes\", \"Adaboost Classifier\", \"Bagging Classifier\", \"Extra Trees Classifier\"],\n",
    "    \"Abhishek\": [\"Decision Tree\", \"Random Forest\", \"Gradient Boosting\", \"Support Vector Machine\", \"K-Nearest Neighbors\", \"Naive Bayes\", \"MLP Classifier\", \"Bagging Classifier\", \"Extra Trees Classifier\"],\n",
    "    \"Sunamdha\": [\"Support Vector Machine\", \"K-Nearest Neighbors\", \"MLP Classifier\", \"Bagging Classifier\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now random voice comes, then predicted across all models and score taken out. >=3 scores set aside. Then check how many of these shortlist models \n",
    "# match with dictionary models. The more interection name choose as user who spoke.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Decision Tree' saved in 'selective_models/'.\n",
      "Model 'Random Forest' saved in 'selective_models/'.\n",
      "Model 'Gradient Boosting' saved in 'selective_models/'.\n",
      "Model 'Naive Bayes' saved in 'selective_models/'.\n",
      "Model 'Adaboost Classifier' not found in trained_models dictionary.\n",
      "Model 'Bagging Classifier' saved in 'selective_models/'.\n",
      "Model 'Extra Trees Classifier' saved in 'selective_models/'.\n"
     ]
    }
   ],
   "source": [
    "# best models for Arun - \n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"selective_models/\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Assuming best_models is a list containing the names of the best models\n",
    "best_model_names = [\"Decision Tree\", \"Random Forest\", \"Gradient Boosting\", \"Naive Bayes\", \"Adaboost Classifier\", \"Bagging Classifier\", \"Extra Trees Classifier\"]\n",
    "\n",
    "# Save the best models with the specified prefix\n",
    "for model_name in best_model_names:\n",
    "    # Check if the model name exists in the trained_models dictionary\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        # Adjust the naming convention as needed\n",
    "        model_filename = os.path.join(folder_path, f\"Arun-{model_name}-Pronounciation.pkl\")\n",
    "        joblib.dump(model, model_filename)\n",
    "        print(f\"Model '{model_name}' saved in '{folder_path}'.\")\n",
    "    else:\n",
    "        print(f\"Model '{model_name}' not found in trained_models dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all pronounciation related models - \n",
    "# once loaded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training all models on MFCC and Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_with_mean(sequences, max_length):\n",
    "    padded_sequences = np.zeros((len(sequences), max_length))\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        if seq_len > 0:\n",
    "            mean_value = np.mean(seq)\n",
    "            padded_sequences[i, :seq_len] = seq\n",
    "            padded_sequences[i, seq_len:] = mean_value\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      1.00      1.00        15\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "\n",
      "Training and evaluating Decision Tree...\n",
      "Accuracy: 0.88\n",
      "Confusion Matrix:\n",
      "[[14  1  0]\n",
      " [ 2 13  0]\n",
      " [ 1  1 10]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.82      0.93      0.88        15\n",
      "        Arun       0.87      0.87      0.87        15\n",
      "    Sunamdha       1.00      0.83      0.91        12\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.90      0.88      0.88        42\n",
      "weighted avg       0.89      0.88      0.88        42\n",
      "\n",
      "\n",
      "Training and evaluating Random Forest...\n",
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      1.00      1.00        15\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Accuracy: 0.90\n",
      "Confusion Matrix:\n",
      "[[14  1  0]\n",
      " [ 1 13  1]\n",
      " [ 1  0 11]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.88      0.93      0.90        15\n",
      "        Arun       0.93      0.87      0.90        15\n",
      "    Sunamdha       0.92      0.92      0.92        12\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.91      0.91      0.91        42\n",
      "weighted avg       0.91      0.90      0.90        42\n",
      "\n",
      "\n",
      "Training and evaluating Support Vector Machine...\n",
      "Accuracy: 0.71\n",
      "Confusion Matrix:\n",
      "[[ 6  6  3]\n",
      " [ 0 13  2]\n",
      " [ 0  1 11]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      0.40      0.57        15\n",
      "        Arun       0.65      0.87      0.74        15\n",
      "    Sunamdha       0.69      0.92      0.79        12\n",
      "\n",
      "    accuracy                           0.71        42\n",
      "   macro avg       0.78      0.73      0.70        42\n",
      "weighted avg       0.79      0.71      0.69        42\n",
      "\n",
      "\n",
      "Training and evaluating K-Nearest Neighbors...\n",
      "Accuracy: 0.90\n",
      "Confusion Matrix:\n",
      "[[14  1  0]\n",
      " [ 0 13  2]\n",
      " [ 0  1 11]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      0.93      0.97        15\n",
      "        Arun       0.87      0.87      0.87        15\n",
      "    Sunamdha       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.90      0.91      0.90        42\n",
      "weighted avg       0.91      0.90      0.91        42\n",
      "\n",
      "\n",
      "Training and evaluating Naive Bayes...\n",
      "Accuracy: 0.93\n",
      "Confusion Matrix:\n",
      "[[13  2  0]\n",
      " [ 1 14  0]\n",
      " [ 0  0 12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.93      0.87      0.90        15\n",
      "        Arun       0.88      0.93      0.90        15\n",
      "    Sunamdha       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.93      0.93      0.93        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n",
      "\n",
      "Training and evaluating MLP Classifier...\n",
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      1.00      1.00        15\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "\n",
      "Training and evaluating AdaBoost Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AdjustedPythonVirtEnv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "Confusion Matrix:\n",
      "[[13  2  0]\n",
      " [11  4  0]\n",
      " [ 3  0  9]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.48      0.87      0.62        15\n",
      "        Arun       0.67      0.27      0.38        15\n",
      "    Sunamdha       1.00      0.75      0.86        12\n",
      "\n",
      "    accuracy                           0.62        42\n",
      "   macro avg       0.72      0.63      0.62        42\n",
      "weighted avg       0.70      0.62      0.60        42\n",
      "\n",
      "\n",
      "Training and evaluating Bagging Classifier...\n",
      "Accuracy: 0.98\n",
      "Confusion Matrix:\n",
      "[[14  0  1]\n",
      " [ 0 15  0]\n",
      " [ 0  0 12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      0.93      0.97        15\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.98        42\n",
      "   macro avg       0.97      0.98      0.98        42\n",
      "weighted avg       0.98      0.98      0.98        42\n",
      "\n",
      "\n",
      "Training and evaluating Extra Trees Classifier...\n",
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       1.00      1.00      1.00        15\n",
      "        Arun       1.00      1.00      1.00        15\n",
      "    Sunamdha       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "\n",
      "Training and evaluating Quadratic Discriminant Analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AdjustedPythonVirtEnv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "Confusion Matrix:\n",
      "[[12  1  2]\n",
      " [ 1 10  4]\n",
      " [ 0  0 12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abhishek       0.92      0.80      0.86        15\n",
      "        Arun       0.91      0.67      0.77        15\n",
      "    Sunamdha       0.67      1.00      0.80        12\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.83      0.82      0.81        42\n",
      "weighted avg       0.84      0.81      0.81        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "df_vector = df.copy()\n",
    "\n",
    "# Assuming df is your DataFrame with 'Vector' and 'Label' columns\n",
    "X_mfcc = df_vector['MFCC'].values\n",
    "X_vector = df_vector['Vector'].values\n",
    "y = df_vector['Name']\n",
    "\n",
    "max_len_mfcc = max(len(x) for x in X_mfcc)\n",
    "max_len_vector = max(len(x) for x in X_vector)\n",
    "max_len = max(max_len_mfcc, max_len_vector)\n",
    "\n",
    "X_mfcc_padded = pad_sequences_with_mean(X_mfcc, max_len)\n",
    "X_vector_padded = pad_sequences_with_mean(X_vector, max_len)\n",
    "\n",
    "# Concatenate 'MFCC' and 'Vector' arrays along the second axis\n",
    "X_combined = np.concatenate([X_mfcc_padded, X_vector_padded], axis=1)\n",
    "\n",
    "\n",
    "# Define a custom padding function\n",
    "def pad_sequences_with_mean(sequences, max_length):\n",
    "    padded_sequences = np.zeros((len(sequences), max_length))\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        if seq_len > 0:\n",
    "            mean_value = np.mean(seq)\n",
    "            padded_sequences[i, :seq_len] = seq\n",
    "            padded_sequences[i, seq_len:] = mean_value\n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "# Find the maximum length of sequences\n",
    "# max_length = max(len(seq) for seq in X)\n",
    "\n",
    "# # Pad the sequences with the mean value\n",
    "# X_padded = pad_sequences_with_mean(X, max_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=23)\n",
    "\n",
    "# Initialize various classifiers and create a dictionary to store trained models\n",
    "trained_models = {}\n",
    "\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'MLP Classifier': MLPClassifier(),\n",
    "    'AdaBoost Classifier': AdaBoostClassifier(),\n",
    "    'Bagging Classifier': BaggingClassifier(),\n",
    "    'Extra Trees Classifier': ExtraTreesClassifier(),\n",
    "    'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"\\nTraining and evaluating {name}...\")\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Save the trained model in the dictionary\n",
    "    trained_models[name] = classifier\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn import preprocessing\n",
    "import python_speech_features as mfcc\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=25):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfccs.flatten()\n",
    "\n",
    "# Function to extract features (MFCC and delta coefficients)\n",
    "def extract_features(audio, rate):\n",
    "    mfcc_feature = mfcc.mfcc(audio, rate, 0.025, 0.01, 20, nfft=1200, appendEnergy=True)\n",
    "    mfcc_feature = preprocessing.scale(mfcc_feature)\n",
    "    delta = calculate_delta(mfcc_feature)\n",
    "    combined = np.hstack((mfcc_feature, delta))\n",
    "    return combined.flatten()\n",
    "\n",
    "# Function to calculate delta coefficients\n",
    "def calculate_delta(array):\n",
    "    rows, cols = array.shape\n",
    "    deltas = np.zeros((rows, 20))\n",
    "    n = 2\n",
    "    for i in range(rows):\n",
    "        index = []\n",
    "        j = 1\n",
    "        while j <= n:\n",
    "            if i - j < 0:\n",
    "                first = 0\n",
    "            else:\n",
    "                first = i - j\n",
    "            if i + j > rows - 1:\n",
    "                second = rows - 1\n",
    "            else:\n",
    "                second = i + j\n",
    "            index.append((second, first))\n",
    "            j += 1\n",
    "        deltas[i] = (array[index[0][0]] - array[index[0][1]] + (2 * (array[index[1][0]] - array[index[1][1]]))) / 10\n",
    "    return deltas\n",
    "\n",
    "def create_df_test_vec(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    # vector_data = {'Vector': []}\n",
    "    vec_mfcc_data = {'MFCC': [], 'Vector': []}\n",
    "\n",
    "    for file_name in files:\n",
    "        if file_name.lower().endswith(\".wav\"):\n",
    "            # MFCC Dataset\n",
    "            input_file_path = os.path.join(folder_path, file_name)\n",
    "            mfccs = extract_mfcc(input_file_path)\n",
    "            vec_mfcc_data['MFCC'].append(mfccs)\n",
    "\n",
    "            # Vector Dataset\n",
    "            # Load the original audio\n",
    "            sr, audio = read(os.path.join(folder_path, file_name))\n",
    "            # Extract features (MFCC and delta coefficients)\n",
    "            features = extract_features(audio, sr)\n",
    "            # Add vector to the dictionary\n",
    "            vec_mfcc_data['Vector'].append(features)  # Add the vector values\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_test_mfcc_vec = pd.DataFrame(vec_mfcc_data)\n",
    "\n",
    "    return df_test_mfcc_vec\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = \"audio_files/test/test/\"\n",
    "\n",
    "# Create DataFrame\n",
    "df_test_mfcc_vec = create_df_test_vec(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCC</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-773.37177, -773.37177, -773.37177, -773.3717...</td>\n",
       "      <td>[-1.2264953037130666, -1.5692548036474234, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-713.7566, -713.7566, -713.7566, -713.7566, -...</td>\n",
       "      <td>[-1.3419352300619702, -1.6418373323387692, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-769.26025, -769.26025, -769.26025, -769.2602...</td>\n",
       "      <td>[-1.1601130120094152, -1.4955321356818048, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-759.9057, -759.9057, -759.9057, -759.9057, -...</td>\n",
       "      <td>[-1.2181456955416914, -1.5679575195827389, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-827.6323, -827.6323, -827.6323, -827.6323, -...</td>\n",
       "      <td>[-1.0357001428889139, -1.3650344273825437, -1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                MFCC  \\\n",
       "0  [-773.37177, -773.37177, -773.37177, -773.3717...   \n",
       "1  [-713.7566, -713.7566, -713.7566, -713.7566, -...   \n",
       "2  [-769.26025, -769.26025, -769.26025, -769.2602...   \n",
       "3  [-759.9057, -759.9057, -759.9057, -759.9057, -...   \n",
       "4  [-827.6323, -827.6323, -827.6323, -827.6323, -...   \n",
       "\n",
       "                                              Vector  \n",
       "0  [-1.2264953037130666, -1.5692548036474234, -1....  \n",
       "1  [-1.3419352300619702, -1.6418373323387692, -1....  \n",
       "2  [-1.1601130120094152, -1.4955321356818048, -1....  \n",
       "3  [-1.2181456955416914, -1.5679575195827389, -1....  \n",
       "4  [-1.0357001428889139, -1.3650344273825437, -1....  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_mfcc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31920,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 31920 features, but LogisticRegression is expecting 15920 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 27\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Make predictions using the trained models\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m trained_models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Ensure the test_vector has the same shape as the training data\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# For example, you may need to pad or reshape it\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# test_vector_padded = pad_sequences_with_mean(X_test_vec, max_length_test)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Store the predictions in the dictionary\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     predictions[name] \u001b[38;5;241m=\u001b[39m y_pred\n",
      "File \u001b[1;32md:\\AdjustedPythonVirtEnv\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:351\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    350\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 351\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    353\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32md:\\AdjustedPythonVirtEnv\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:332\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    329\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    330\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 332\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32md:\\AdjustedPythonVirtEnv\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\AdjustedPythonVirtEnv\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 31920 features, but LogisticRegression is expecting 15920 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "X_mfcc = df_test_mfcc_vec['MFCC'].values\n",
    "X_vector = df_test_mfcc_vec['Vector'].values\n",
    "\n",
    "max_len_mfcc = max(len(x) for x in X_mfcc)\n",
    "max_len_vector = max(len(x) for x in X_vector)\n",
    "max_len = max(max_len_mfcc, max_len_vector)\n",
    "\n",
    "X_mfcc_padded = pad_sequences_with_mean(X_mfcc, max_len)\n",
    "X_vector_padded = pad_sequences_with_mean(X_vector, max_len)\n",
    "\n",
    "# Concatenate 'MFCC' and 'Vector' arrays along the second axis\n",
    "X_combined = np.concatenate([X_mfcc_padded, X_vector_padded], axis=1)\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store predictions\n",
    "predictions = {}\n",
    "\n",
    "# Make predictions using the trained models\n",
    "for name, model in trained_models.items():\n",
    "    # Ensure the test_vector has the same shape as the training data\n",
    "    # For example, you may need to pad or reshape it\n",
    "    # test_vector_padded = pad_sequences_with_mean(X_test_vec, max_length_test)\n",
    "    \n",
    "    # Make the prediction\n",
    "    y_pred = model.predict(X_combined)\n",
    "    \n",
    "    # Store the predictions in the dictionary\n",
    "    predictions[name] = y_pred\n",
    "\n",
    "# Create a DataFrame from the predictions\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Display the predicted usernames\n",
    "df_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdjustedPythonVirtEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
